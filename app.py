import os
import re
import streamlit as st
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
from googleapiclient.discovery import build

# =====================
# C·∫§U H√åNH API
# =====================
API_KEY = 'AIzaSyBce8zkHaX5SC42SM2tBKu_RZzrMY46UUo'  # ‚Üê Nh·∫≠p YouTube API Key t·∫°i ƒë√¢y
youtube = build('youtube', 'v3', developerKey=API_KEY)

# =====================
# T·∫¢I M√î H√åNH TI·∫æNG VI·ªÜT
# =====================
MODEL_NAME = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
labels = ['ti√™u c·ª±c', 'trung t√≠nh', 't√≠ch c·ª±c']

def analyze_sentiment(comment):
    encoded_input = tokenizer(comment, return_tensors='pt', truncation=True)
    with torch.no_grad():
        output = model(**encoded_input)
    scores = F.softmax(output.logits, dim=1)
    return labels[torch.argmax(scores).item()]

def is_spam(comment):
    return bool(re.search(r"(http|www\.|zalo|telegram|free|subscribe|vay ti·ªÅn|ki·∫øm ti·ªÅn)", comment.lower()))

def search_videos(keyword, max_results=3):
    request = youtube.search().list(q=keyword, part='id', type='video', maxResults=max_results)
    response = request.execute()
    return [item['id']['videoId'] for item in response['items']]

def get_comments(video_id):
    comments = []
    request = youtube.commentThreads().list(part='snippet', videoId=video_id, maxResults=100, textFormat='plainText')
    while request:
        response = request.execute()
        for item in response['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            comments.append(comment)
        request = youtube.commentThreads().list_next(request, response)
    return comments

def classify_all_comments(all_comments):
    results = []
    for comment in all_comments:
        label = "spam" if is_spam(comment) else analyze_sentiment(comment)
        results.append({"comment": comment, "label": label})
    return pd.DataFrame(results)

def save_to_excel(df):
    from io import BytesIO
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name="T·∫•t c·∫£", index=False)
        for label in df["label"].unique():
            df[df["label"] == label].to_excel(writer, sheet_name=label.capitalize(), index=False)
    output.seek(0)
    return output

# =====================
# GIAO DI·ªÜN WEB
# =====================
st.set_page_config(page_title="Ph√¢n t√≠ch b√¨nh lu·∫≠n YouTube", layout="centered")
st.title("üí¨ Ph√¢n t√≠ch b√¨nh lu·∫≠n YouTube (Ti·∫øng Vi·ªát)")
st.markdown("·ª®ng d·ª•ng s·ª≠ d·ª•ng m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ ph√¢n lo·∫°i c·∫£m x√∫c c·ªßa b√¨nh lu·∫≠n.")

keyword = st.text_input("üîç Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm video:")

if st.button("Ph√¢n t√≠ch"):
    if not keyword:
        st.warning("Vui l√≤ng nh·∫≠p t·ª´ kh√≥a.")
    else:
        with st.spinner("ƒêang t√¨m video v√† t·∫£i b√¨nh lu·∫≠n..."):
            video_ids = search_videos(keyword)
            all_comments = []
            for vid in video_ids:
                comments = get_comments(vid)
                all_comments.extend(comments)

        if not all_comments:
            st.error("Kh√¥ng t√¨m th·∫•y b√¨nh lu·∫≠n.")
        else:
            st.success(f"ƒê√£ t·∫£i {len(all_comments)} b√¨nh lu·∫≠n. ƒêang ph√¢n t√≠ch...")
            df = classify_all_comments(all_comments)

            st.dataframe(df.head(10))
            file = save_to_excel(df)

            st.download_button(
                label="üì• T·∫£i file Excel",
                data=file,
                file_name="comments_vi.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )